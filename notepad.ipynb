{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, so we're going to implement the episodic learning algorithm I devised...\n",
    "\n",
    "High level steps:\n",
    "1. Sample 2N sequences from the dataset\n",
    "2. Generate 2N * X insertions of thought tokens into each sequence.\n",
    "3. Compute the difference in log likelihood of the sequence with and without the thought tokens\n",
    "4. Remove the worst N sequences\n",
    "\n",
    "Q: Could we do this on a per-token basis instead of a per-sequence basis?\n",
    "i.e. could we insert groups of thought tokens and then remove the worst ones from the sequence as we go?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some thoughts:\n",
    "1. Say we're going to *strictly* insert 32 Thought Tokens per sequence.\n",
    "2. First, we run forward pass, then the 32 locations with highest likelihood of predicting a thought token are selected.\n",
    "3.  - In theory, one token being inserted will affect the likelihood of other tokens being inserted. This would influence doubles or triples for example...\n",
    "    - Another method would be to 'run' through the sequence, inserting thought tokens one at a time as if the model were properly generating them as it went.\n",
    "        - This is probably the *correct* way I should do things...\n",
    "\n",
    "\n",
    "Idea:\n",
    "1. Run 32 sequences through model\n",
    "2. Find 32 first-candidates for a thought token\n",
    "3. Insert thought tokens and run through model again\n",
    "4. Compute the difference in log likelihood of the sequence with and without the thought tokens\n",
    "5. Remove the worst 16 thought-tokens that were inserted\n",
    "6. Repeat\n",
    "---\n",
    "So, for 16 forward passes, we'll have 16*16=256 thought tokens inserted in total, average 8 per sequence....\n",
    "\n",
    "This phase we can do very quickly. If we're not masking, and not computing gradients, we can hit ~200k tokens per second processed, so we can do about 15 forward passes in a single second in theory.\n",
    "\n",
    "Then, the actual foward-backward passes...\n",
    "- We can use the pre-computed thought tokens as a strong baseline instead of an empty initial state.\n",
    "- Choose depth to unroll, run forward pass, compute loss, compute gradients, update parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Ok, this flow actually seems pretty good to me:\n",
    "1. We're not throwing away sequences, only individual thought tokens, which means we're hopefully not gonna delete 'hard to learn' sequences.\n",
    "2. By doing 'per-token', we're \"reinforcing\" good thought token predictions and punishing bad ones.\n",
    "3. By running-through entire sequence for insertion, we're actually taking into account each token that's added.\n",
    "4. Finally, by using a strong initialization for our actual forward-backward pass ones, while gradients aren't flowing super deeply backwards we're getting some of the benefit of many iterated thought tokens.\n",
    "\n",
    "\n",
    "The two hard bits:\n",
    "1. The 'fastfoward' generation algorithm, where we skip through but insert thought-tokens along the way. Gotta keep that performant but that's not going to be easy.\n",
    "2. Choosing where to insert a thought token if the model really doesn't want to insert one. Ideally, we start by choosing the first location where a thought token is actually predicted as #1, but if that doesn't occur we still need to choose a thought token for somewhere....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from core.model import THOUGHT_TOKEN_ID\n",
    "# choose a location to insert a thought token\n",
    "THOUGHT_TOKEN_ID = 4\n",
    "\n",
    "# hopefully with nograd this won't be too slow or memory intensive\n",
    "@torch.no_grad()\n",
    "def insert_a_thought_token(logits: torch.Tensor, tokens: torch.Tensor):\n",
    "    sorted_probs, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "    ranks = torch.where(sorted_indices == THOUGHT_TOKEN_ID)[1]\n",
    "    # insert a thought token at the location of the lowest rank.\n",
    "    index = torch.argmin(ranks)\n",
    "\n",
    "    tokens_out = torch.cat([tokens[:, :index], THOUGHT_TOKEN_ID, tokens[:, index:]], dim=1)\n",
    "    \n",
    "    return tokens_out, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def insert_a_thought_token(logits: torch.Tensor, tokens: torch.Tensor):\n",
    "    batch_size, seq_len = tokens.size()\n",
    "    sorted_probs, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "\n",
    "    # find the ranks of THOUGHT_TOKEN_ID for each element in the batch\n",
    "    ranks = (sorted_indices == THOUGHT_TOKEN_ID).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    # insert a thought token at the location of the lowest rank.\n",
    "    min_ranks, min_indices = torch.min(ranks, dim=-1)\n",
    "\n",
    "    # build a tensor of THOUGHT_TOKEN_ID repeated along batch_size\n",
    "    tokens_out = []\n",
    "    for b in range(batch_size):\n",
    "        index = min_indices[b]\n",
    "        tokens_out.append(torch.cat([tokens[b, :index], THOUGHT_TOKEN_ID, tokens[b, index:]], dim=0))\n",
    "    \n",
    "    # pad sequences to the same length\n",
    "    tokens_out = torch.nn.utils.rnn.pad_sequence(tokens_out, batch_first=True)\n",
    "    \n",
    "    return tokens_out, min_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alex/projects/thought-tokens/notepad.ipynb Cell 6\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokens_original \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokens_trial, index \u001b[39m=\u001b[39m insert_a_thought_token(logits, tokens_original)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/alex/projects/thought-tokens/notepad.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m tokens_out \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     index \u001b[39m=\u001b[39m min_indices[b]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     tokens_out\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mcat([tokens[b, :index], THOUGHT_TOKEN_ID, tokens[b, index:]], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alex/projects/thought-tokens/notepad.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# pad sequences to the same length\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "tokens_original = torch.tensor([[0, 1, 3, 2, 3, 1, 0, 1]])\n",
    "logits = torch.randn(1, 8, 5)\n",
    "\n",
    "tokens_trial, index = insert_a_thought_token(logits, tokens_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 0, 1, 3, 2, 3, 1, 0, 1]]), tensor(0))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_trial, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the flow\n",
    "\n",
    "\n",
    "\n",
    "    tokens_original = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "\n",
    "    logits_original, losses = model(tokens_original)\n",
    "\n",
    "    episode_trial_tokens, episode_token_insertion_index = insert_a_thought_token(logits_original, tokens_original)\n",
    "\n",
    "    logits_episode_trial, losses_trial = model(episode_trial_tokens)\n",
    "\n",
    "    completion_perplexity_original = torch.exp(-torch.mean(losses_trial[episode_token_insertion_index:]))\n",
    "    completion_perplexity_trial = torch.exp(-torch.mean(losses_trial[episode_token_insertion_index + 1:]))\n",
    "\n",
    "    diff = completion_perplexity_trial - completion_perplexity_original\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
