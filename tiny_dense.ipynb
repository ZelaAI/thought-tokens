{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file = hf_hub_download('alexedw/dense-train-masked-between-tokens', 'model_state.pt', revision='1500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "from core.model import GPT, GPTConfig, Tokenizer, THOUGHT_TOKEN_ID\n",
    "import torch\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "model = GPT(GPTConfig.from_pretrained('EleutherAI/pythia-410m'))\n",
    "state_dict = torch.load(file, map_location=torch.device('cpu'))\n",
    "\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "state_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_encode=\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2023 Zela Labs\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer.encode(f'{text_to_encode}<|dense|><|dense|><|dense|>').unsqueeze(0)\n",
    "\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_new = model.generate(torch.tensor([[50277, 50277, 50277]]), dense_input=dense[:, -3:, :], max_new_tokens=25, temperature=0.0001)\n",
    "tokens_new, tokenizer.decode(tokens_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_new = model.generate(torch.tensor([[50277, 50277, 50277]]), max_new_tokens=15, temperature=0.0001)\n",
    "tokens_new, tokenizer.decode(tokens_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test number 2... encoding a memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_encode=\"\"\"My name is Alex. Alex Alex Alex loves the colour blue blue blue\"\"\"\n",
    "tokens = tokenizer.encode(f'{text_to_encode}<|dense|><|dense|><|dense|>').unsqueeze(0)\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))\n",
    "\n",
    "name_one_dense = dense[:, -3:, :] * 2\n",
    "\n",
    "text_to_encode=\"\"\"My name is Sarah. Sarah Sarah Sarah loves the colour green green green\"\"\"\n",
    "tokens = tokenizer.encode(f'{text_to_encode}<|dense|><|dense|><|dense|>').unsqueeze(0)\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))\n",
    "logits, dense, loss = model(tokens, model.create_dense_inputs(tokens, dense))\n",
    "\n",
    "name_two_dense = dense[:, -3:, :] * 2\n",
    "\n",
    "question_one_tokens = tokenizer.encode(\"Hi there, I'd like to introduce myself, my name is<|dense|><|dense|><|dense|>\").unsqueeze(0)\n",
    "question_two_tokens = tokenizer.encode(\"Hi there, my favourite colour is<|dense|><|dense|><|dense|>\").unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I'd like to introduce myself, my name is<|dense|><|dense|><|dense|>. I'm a student at the University of the Arts in London. I'm a student in the English department. I'm\n"
     ]
    }
   ],
   "source": [
    "dense_inputs = model.create_dense_inputs(question_one_tokens)\n",
    "dense_inputs[:, -3:, :] = name_one_dense\n",
    "tokens_new = model.generate(question_one_tokens, dense_input=dense_inputs, max_new_tokens=25, temperature=0.0001)\n",
    "print(tokenizer.decode(tokens_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, I'd like to introduce myself, my name is<|dense|><|dense|><|dense|> green. I'm a student at the University of the Arts in London. I'm a student in the English department and I\n"
     ]
    }
   ],
   "source": [
    "dense_inputs = model.create_dense_inputs(question_one_tokens)\n",
    "dense_inputs[:, -3:, :] = name_two_dense\n",
    "tokens_new = model.generate(question_one_tokens, dense_input=dense_inputs, max_new_tokens=25, temperature=0.0001)\n",
    "print(tokenizer.decode(tokens_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, my favourite colour is<|dense|><|dense|><|dense|>. I love it so much. I have a few of my own and I love them. I have a few of my\n"
     ]
    }
   ],
   "source": [
    "dense_inputs = model.create_dense_inputs(question_two_tokens)\n",
    "dense_inputs[:, -3:, :] = name_one_dense\n",
    "tokens_new = model.generate(question_two_tokens, dense_input=dense_inputs, max_new_tokens=25, temperature=0.0001)\n",
    "print(tokenizer.decode(tokens_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, my favourite colour is<|dense|><|dense|><|dense|> green. I love the colour of the sky and the sky is always green. I love the colour of the sky and the\n"
     ]
    }
   ],
   "source": [
    "dense_inputs = model.create_dense_inputs(question_two_tokens)\n",
    "dense_inputs[:, -3:, :] = name_two_dense\n",
    "tokens_new = model.generate(question_two_tokens, dense_input=dense_inputs, max_new_tokens=25, temperature=0.0001)\n",
    "print(tokenizer.decode(tokens_new[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
