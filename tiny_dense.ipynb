{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0d7d4db194555a03b306dde0469bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file = hf_hub_download('alexedw/dense-train-masked-between-tokens', 'model_state.pt', revision='1500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alex/.cache/huggingface/hub/models--alexedw--pythia-410-dense-test-1/snapshots/c930775cfacb01fcf2625fbfeff752c428a9ca54/model_state.pt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 405.33M\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "from core.model import GPT, GPTConfig, Tokenizer, DENSE_TOKEN_ID\n",
    "import torch\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "model = GPT(GPTConfig.from_pretrained('EleutherAI/pythia-410m'))\n",
    "state_dict = torch.load(file, map_location=torch.device('cpu'))\n",
    "\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "state_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = torch.tensor([tokenizer.encode('My name is Alex Dog Dog Dog<|dense|><|dense|>')])\n",
    "\n",
    "logits, dense, loss = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50277]), '<|dense|>')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last Token = tokens[0,5] # Last Logits = logits[0,5]\n",
    "token = model.sample_top_p(logits[0,5], top_p=0.0, temperature=0.001)\n",
    "token, tokenizer.decode(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50277, 21048, 50277,   434,   367, 11345,   187,   187,   510,  4370,\n",
       "            434, 44802,   310,   247,  1077,  1774]]),\n",
       " \"<|dense|> Dog<|dense|>'s Paws\\n\\nThe dog's paw is a very important\")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(idx, max_new_tokens, dense_input=None, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    if dense_input is None:\n",
    "        dense_input = model.create_dense_inputs(idx)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        logits, dense_out, _ = model(idx, dense=dense_input)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :]\n",
    "        # sample from the top-p distribution\n",
    "        idx_next = model.sample_top_p(logits, temperature, top_p)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        dense_input = model.create_dense_inputs(idx, DENSE_TOKEN_ID, dense_out)\n",
    "\n",
    "    return idx\n",
    "\n",
    "tokens_new = generate(torch.tensor([[50277,50277]]), dense_input=dense[:, -2:, :], max_new_tokens=15, temperature=0.0001)\n",
    "tokens_new, tokenizer.decode(tokens_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50277, 29408, 50277,    15,   187,   187,   510,   806,  2181,   309,\n",
       "            858,   369,   281,   564,   281,   253]]),\n",
       " '<|dense|>cie<|dense|>.\\n\\nThe first thing I did was to go to the')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_new = generate(torch.tensor([[50277]]), max_new_tokens=15, temperature=0.0001)\n",
    "tokens_new, tokenizer.decode(tokens_new[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
