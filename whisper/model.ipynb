{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/.pyenv/versions/3.10.6/lib/python3.10/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 384)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "from whisper.audio import log_mel_spectrogram, load_audio, pad_or_trim\n",
    "from whisper.decoding import DecodingOptions\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "mel = log_mel_spectrogram(load_audio('./audio.mp3'), padding=30 * 16000)\n",
    "\n",
    "N_FRAMES = 30 * 16000 // 160  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "mel_segment = pad_or_trim(mel, N_FRAMES).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecodingResult(audio_features=tensor([[ 0.1039,  0.0522,  0.2074,  ..., -0.1066,  0.1678,  0.0533],\n",
       "         [ 0.5284,  2.0875, -0.3823,  ..., -1.8898, -0.4868,  0.3108],\n",
       "         [-0.9325,  1.3111, -1.5403,  ..., -1.6143,  1.0681, -0.4109],\n",
       "         ...,\n",
       "         [ 0.7557, -1.7804,  0.2187,  ..., -0.1072, -0.5025,  0.5058],\n",
       "         [-0.0753, -0.4694,  0.1547,  ...,  0.6741,  0.0419,  0.3270],\n",
       "         [ 0.1310, -0.0847, -1.4486,  ...,  0.0707, -0.5199, -0.2022]]), language='en', language_probs=None, tokens=[50364, 34439, 278, 11, 294, 264, 787, 2020, 365, 597, 321, 366, 412, 1974, 5922, 11, 37761, 490, 881, 50644, 50644, 498, 406, 490, 439, 264, 8609, 293, 27831, 10379, 294, 264, 14414, 13, 50844], text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition.', avg_logprob=-0.182177291976081, no_speech_prob=0.0037185135297477245, temperature=0.0, compression_ratio=1.345132743362832)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(mel_segment, DecodingOptions(fp16=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecodingOptions(task='transcribe', language=None, temperature=0.0, sample_len=None, best_of=None, beam_size=None, patience=None, length_penalty=None, prompt=None, prefix=None, suppress_tokens='-1', suppress_blank=True, without_timestamps=False, max_initial_timestamp=1.0, fp16=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecodingOptions(fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_logits = model.encoder(mel_segment)\n",
    "tokens = torch.randint(0, 50000, (1,15))\n",
    "decoder_logits = model.decoder(tokens, encoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a fucking model.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# nn.Conv1d, nn.Linear, nn.LayerNorm\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from whisper.model import MultiHeadAttention as MultiHeadAttentionOG\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "        \n",
    "    def forward(self, x, xa=None, causal=False):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x if xa is None else xa) # TODO: Cache these for `xa` \n",
    "        v = self.value(x if xa is None else xa)\n",
    "        \n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) # BS, n_head, seq_len, head_size\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=causal).permute(0, 2, 1, 3)\n",
    "        \n",
    "        return self.out(y.flatten(start_dim=2))\n",
    "\n",
    "\n",
    "mha = MultiHeadAttentionOG(128, 8)\n",
    "mhaalex = MultiHeadAttention(128, 8)\n",
    "mhaalex.load_state_dict(mha.state_dict())\n",
    "\n",
    "logits = torch.randn((2,10,128))\n",
    "logits2 = torch.randn((2,25,128))\n",
    "\n",
    "o1, o2 = mha(logits)\n",
    "o1_alex = mhaalex(logits)\n",
    "print(torch.allclose(o1, o1_alex))\n",
    "\n",
    "o1, o2 = mha(logits, logits2)\n",
    "o1_alex = mhaalex(logits, logits2)\n",
    "print(torch.allclose(o1, o1_alex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = nn.LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_state * 4), nn.GELU(), nn.Linear(n_state * 4, n_state)\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None):\n",
    "        x = x + self.attn(self.attn_ln(x), causal=self.cross_attn is not None)\n",
    "    \n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa)\n",
    "    \n",
    "        return x + self.mlp(self.mlp_ln(x))\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)])\n",
    "        self.ln_post = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = (x + self.positional_embedding)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return self.ln_post(x)\n",
    "\n",
    "alex_encoder = AudioEncoder(n_mels=model.dims.n_mels, n_ctx=model.dims.n_audio_ctx, n_state=model.dims.n_audio_state, n_head=model.dims.n_audio_head, n_layer=model.dims.n_audio_layer)\n",
    "\n",
    "alex_encoder.load_state_dict(model.encoder.state_dict())\n",
    "\n",
    "alex_encoder_logits = alex_encoder(mel_segment)\n",
    "torch.allclose(alex_encoder_logits, encoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head, cross_attention=True) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa):\n",
    "        x = (self.token_embedding(x) + self.positional_embedding[:x.shape[-1]])\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight, 0, 1))\n",
    "\n",
    "        return logits\n",
    "\n",
    "text_decoder = TextDecoder(n_vocab=model.dims.n_vocab, n_ctx=model.dims.n_text_ctx, n_state=model.dims.n_text_state, n_head=model.dims.n_text_head, n_layer=model.dims.n_text_layer)\n",
    "text_decoder.load_state_dict(model.decoder.state_dict())\n",
    "\n",
    "alex_decoder_logits = text_decoder(tokens, alex_encoder_logits)\n",
    "torch.allclose(alex_decoder_logits, decoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(logits, temperature, top_p):\n",
    "    \"\"\"\n",
    "    Takes a list of logits returns a sampled token\n",
    "    from the top-p distribution for each sequence where mask[x] == 1.\n",
    "    \"\"\"\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(F.softmax(logits, dim=-1), dim=-1, descending=True)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_keep = torch.cumsum(sorted_logits, dim=-1) <= top_p\n",
    "    \n",
    "    # Shift the indices to the right to keep also the first token above the threshold\n",
    "    sorted_indices_to_keep = torch.roll(sorted_indices_to_keep, shifts=1, dims=-1)\n",
    "    sorted_indices_to_keep[..., 0] = 1  # Keep first token always\n",
    "\n",
    "    mask_zero = torch.zeros_like(logits, dtype=torch.bool).scatter_(-1, sorted_indices, sorted_indices_to_keep)\n",
    "    logits = logits * mask_zero\n",
    "\n",
    "    # Sample tokens only for active positions\n",
    "    return torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "\n",
    "\n",
    "def generate(tokens, encoder_logits, max_new_tokens=40):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        logits = text_decoder(tokens, encoder_logits)\n",
    "        \n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # sample from the top-p distribution\n",
    "        token_next = sample_top_p(logits, 0.1, 0.1)\n",
    "        \n",
    "        # append sampled index to the running sequence and continue\n",
    "        tokens = torch.cat((tokens, token_next), dim=1)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.tokenizer import get_tokenizer\n",
    "tokenizer = get_tokenizer(model.is_multilingual, language='en', task='transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate(torch.tensor([tokenizer.sot_sequence], dtype=torch.int64), alex_encoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Whisper\n",
    "\n",
    "my_model = Whisper.load_from_pretrained(\"tiny\")\n",
    "\n",
    "encoder_logits = my_model.encoder(mel_segment)\n",
    "tokens = my_model.generate(torch.tensor([tokenizer.sot_sequence], dtype=torch.int64), alex_encoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
