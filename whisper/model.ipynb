{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 384)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "from whisper.audio import log_mel_spectrogram, load_audio, pad_or_trim\n",
    "from whisper.decoding import DecodingOptions\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "mel = log_mel_spectrogram(load_audio('./audio.mp3'), padding=30 * 16000)\n",
    "\n",
    "N_FRAMES = 30 * 16000 // 160  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "mel_segment = pad_or_trim(mel, N_FRAMES).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecodingResult(audio_features=tensor([[ 0.1039,  0.0522,  0.2074,  ..., -0.1066,  0.1678,  0.0533],\n",
       "         [ 0.5284,  2.0875, -0.3823,  ..., -1.8898, -0.4868,  0.3108],\n",
       "         [-0.9325,  1.3111, -1.5403,  ..., -1.6143,  1.0681, -0.4109],\n",
       "         ...,\n",
       "         [ 0.7557, -1.7804,  0.2187,  ..., -0.1072, -0.5025,  0.5058],\n",
       "         [-0.0753, -0.4694,  0.1547,  ...,  0.6741,  0.0419,  0.3270],\n",
       "         [ 0.1310, -0.0847, -1.4486,  ...,  0.0707, -0.5199, -0.2022]]), language='en', language_probs=None, tokens=[50364, 34439, 278, 11, 294, 264, 787, 2020, 365, 597, 321, 366, 412, 1974, 5922, 11, 37761, 490, 881, 50644, 50644, 498, 406, 490, 439, 264, 8609, 293, 27831, 10379, 294, 264, 14414, 13, 50844], text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition.', avg_logprob=-0.182177291976081, no_speech_prob=0.0037185135297477245, temperature=0.0, compression_ratio=1.345132743362832)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(mel_segment, DecodingOptions(fp16=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_logits = model.encoder(mel_segment)\n",
    "tokens = torch.randint(0, 50000, (1,15))\n",
    "decoder_logits = model.decoder(tokens, encoder_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a fucking model.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# nn.Conv1d, nn.Linear, nn.LayerNorm\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.model import MultiHeadAttention\n",
    "\n",
    "mha = MultiHeadAttention(128, 8)\n",
    "logits = torch.randn((2,10,128))\n",
    "o1, o2 = mha(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "class MultiHeadAttentionAlex(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "        \n",
    "    def forward(self, x, xa=None, mask=None):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x if xa is None else xa)\n",
    "        v = self.value(x if xa is None else xa)\n",
    "        \n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) # BS, n_head, seq_len, head_size\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask).permute(0, 2, 1, 3)\n",
    "        \n",
    "        return self.out(y.flatten(start_dim=2))\n",
    "\n",
    "mhaalex = MultiHeadAttentionAlex(128, 8)\n",
    "mhaalex.load_state_dict(mha.state_dict())\n",
    "\n",
    "o1_alex = mhaalex(logits)\n",
    "\n",
    "torch.allclose(o1, o1_alex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = nn.LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_state * 4), nn.GELU(), nn.Linear(n_state * 4, n_state)\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask)\n",
    "    \n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa)\n",
    "    \n",
    "        return x + self.mlp(self.mlp_ln(x))\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)])\n",
    "        self.ln_post = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = (x + self.positional_embedding)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return self.ln_post(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
